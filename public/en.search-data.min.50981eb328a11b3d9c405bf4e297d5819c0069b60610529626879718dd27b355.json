[{"id":0,"href":"/a-quick-and-easy-way-to-grab-tables-from-websites-with-beautifulsoup-and-pandas-in-python./","title":"A quick and easy way to grab tables from websites with BeautifulSoup and Pandas in Python.","section":"My New Hugo Site","content":"For those of you who are somewhat familiar with BeautifulSoup and are comfortable enough to read throgh the script and comments, the script I used is as follows. The hints in the comments should guide you through the script. If you want a step by step breakdown, I have put it below the whole script.\nHere is the code without comments. (The code with comments is right below)\nimport requests from lxml import html from bs4 import BeautifulSoup import pandas as pd headers_1 = {\u0026#39;Accept-Encoding\u0026#39;: \u0026#39;identity\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)\u0026#39;} connection_one = requests.get(\u0026#39;http://www.onekp.com/public_data.html\u0026#39;, headers = headers_1) soup_object = BeautifulSoup(connection_one.content,features=\u0026#34;lxml\u0026#34;) table = soup_object.find(\u0026#39;table\u0026#39;) rows = table.findAll(lambda tag: tag.name==\u0026#39;tr\u0026#39;) l =[] for tr in rows: td = tr.find_all(\u0026#39;td\u0026#39;) row = [] for tr in td: try: ref = tr.find(\u0026#39;a\u0026#39;,href=True) i = ref[\u0026#39;href\u0026#39;] row.append(i) except: i = tr.text row.append(i) l.append(row) df = pd.DataFrame(l) df.to_csv(\u0026#39;Onekp_data.tsv\u0026#39;,sep=\u0026#39;\\t\u0026#39;) Here is the code with comments.\n# the following libraries are need for this script to work import requests from lxml import html from bs4 import BeautifulSoup import pandas as pd # The following three lines will make the code able to access the file on the internet. The following three lines currently work but I am not sure if the setup is sustainable. It is a patchwork of patchworks that may or may not keep working. headers_1 = {\u0026#39;Accept-Encoding\u0026#39;: \u0026#39;identity\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)\u0026#39;} # this is need to save us from being hit with captcha connection_one = requests.get(\u0026#39;http://www.onekp.com/public_data.html\u0026#39;, headers = headers_1) # this is the website soup_object = BeautifulSoup(connection_one.content,features=\u0026#34;lxml\u0026#34;) # this create a soup object that we will parse table = soup_object.find(\u0026#39;table\u0026#39;) # The table that I am working with has only one so I am using find but you can use findAll to find tables. rows = table.findAll(lambda tag: tag.name==\u0026#39;tr\u0026#39;) # This converts each row in my table into a row that can be parsed. I understand \u0026#39;tr\u0026#39; to mean table row in html but not sure. l =[] # this is the list that we will make a dataframe out of. for tr in rows: # here we are going through the loopable \u0026#39;rows\u0026#39; object that we made a second ago td = tr.find_all(\u0026#39;td\u0026#39;) # This goes through the individual rows in the tr objects and makes the members of the rows loopable. row = [] # this is to hold each new row items and emptied out each time for a new row to come in. for tr in td: # we made an object td that has all the items in each row, but those are html lines and not the information we need or at least it has more information than we really need. #ref= tr.find_all(\u0026#39;a\u0026#39;,href=True) try: # using the try method in python makes it much easier to handle error. Errors such as empty cells, which ironaically I have not handled here but plan to do in the future. ref = tr.find(\u0026#39;a\u0026#39;,href=True) # this takes the individual item in thw row and makes it possible to go through the elements of the item, parsing HTML is wild stuff! i = ref[\u0026#39;href\u0026#39;] # href stand for the hyperlink refernce which is what I need. Otherwise you will only get the text with \u0026#39;tr.text\u0026#39;. Some of my columns have hypterlinks, some of my columns have just text and I need to work around that. row.append(i) # append it to the \u0026#39;row\u0026#39; object that we made above except: # some of the rows are text only so the ones that are rejected are sent below to have their text extracted i = tr.text # extracts text row.append(i) # appends item l.append(row) # append the row df = pd.DataFrame(l) # makes a dataframe df.to_csv(\u0026#39;Onekp_data.tsv\u0026#39;,sep=\u0026#39;\\t\u0026#39;) # this one makes a dataframe in the current directory. I was recently looking for a way to grab a table from an html site from the web as a pandas dataframe and what I found on google was not satisfactory. The fact of the matter is BeautifulSoup is an amazing package that makes a gargantuan task easy but it is not quite a one-liner package like Pandas and needs a little bit of preamble of code to work. So, if what you want to do deviates from tutorials AND if you do not know the aforementioned preamble you will have a not so nice time trying to do something as simple as making a dataframe out of a table on a website.\nSo, I have documented a method that might make it easy for you to get this specific task done.\nLet\u0026rsquo;s go through the steps then.\nImport the packages that we will need. # We will need the following packages for this to work. Use pip to install the ones you are missing with pip install {insert package name here}.\n# the following libraries are need for this script to work import requests from lxml import html from bs4 import BeautifulSoup import pandas as pd Now give the script the ability to servers think it is a browser. # If I understood this correctly, servers can tell if they are being accessed by automated tools and want to discourage this (or something along the lines of that) and as such we have to make our script pretend it is a browser when it contacts the server or url of interest. The code below helps us do this.\nHere, try to leave the headers_1 = {'Accept-Encoding': 'identity', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'} line as is. This gives your script a mask that it can use on the server.\nAs for the url, you can supply your own url as the first parameter to the connection_one object. You will then supply the connection_one object to BeautifulSoup itself.\n# The following three lines will make the code able to access the file on the internet. The following three lines currently work but I am not sure if the setup is sustainable. It is a patchwork of patchworks that may or may not keep working. headers_1 = {\u0026#39;Accept-Encoding\u0026#39;: \u0026#39;identity\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)\u0026#39;} # this is need to save us from being hit with captcha connection_one = requests.get(\u0026#39;http://www.onekp.com/public_data.html\u0026#39;, headers = headers_1) # this is the website soup_object = BeautifulSoup(connection_one.content,features=\u0026#34;lxml\u0026#34;) # this create a soup object that we will parse Get the tables on the website. # You can/should get tables from a webpage ,among others, two of the following ways:\nUsing find\ntable = soup_object.find(\u0026#39;table\u0026#39;) # The table that I am working with has only one so I am using find but you can use findAll to find tables. OR\nThe website in this demo only has one table so find is fine but if your website has multiple tables use findAll (The A is capital).\ntable = soup_object.findAll(\u0026#39;table\u0026#39;) If you had multiple tables, your tables would be indexed into a list and the first table would be accessed as table[0], the second one as table[1] and so on.\nHTML element are strctured/named with a start tag, a end tag and some content in-between as\n\u0026lt;table\u0026gt; {some data here} \u0026lt;/table\u0026gt; And what we are doing with the soup_object.find('{insert tag here}') is to search and locate all the elements named table in our soup object.\nNow to go through a table # rows = table.findAll(lambda tag: tag.name==\u0026#39;tr\u0026#39;) # This converts each row in my table into a row that can be parsed. I understand \u0026#39;tr\u0026#39; to mean table row in html but not sure. Your table in HTML is just like any other table, it is made up of columns and rows. And rows in a HTML table are defined with a \u0026lt;tr\u0026gt; start and a \u0026lt;/tr\u0026gt; end tag and just like a table tag, we can search for a row tag INSIDE of a BeautifulSoup table object.\nSo, this line of code goes through our designated table and find all the row elements and assigns each row to the list called rows, a loopable list.\nFinally going through the cells of the rows. # Now the remainder of the code is somewhat of a monolith that does a few things at the same time. Let\u0026rsquo;s go through it.\nl =[] for tr in rows: td = tr.find_all(\u0026#39;td\u0026#39;) row = [] for tr in td: try: ref = tr.find(\u0026#39;a\u0026#39;,href=True) i = ref[\u0026#39;href\u0026#39;] row.append(i) except: i = tr.text row.append(i) l.append(row) The function of l = [] becomes clear later.\nUsing the for tr in rows line we are going through the rows object we created earlier, i.e. each cell in the row. BUT that is not enough becuase each cell in each row is another HTML element, specifically the td element.\nSo, for each td element we need to find a way to get into the td element and extract meanigful information out of the HTML markup. To do this we return to our good friend find() and supply it with the td parameter as td = tr.find_all('td'). This will give us all the elements in the td tag and put them in the list td for us to loop through.\nNow we create an empty list called row, to keep our td cell elements from each row. Which then sets us up to enter the last loop in our script.\nfor tr in td: try: ref = tr.find(\u0026#39;a\u0026#39;,href=True) i = ref[\u0026#39;href\u0026#39;] row.append(i) except: i = tr.text row.append(i) l.append(row) Here we loop through each cell of each row of each table. We are using the try/except method to filter out both hrefs (hyperlinks) and text. We try to find hyperlinks in the current cell element with ref = tr.find('a',href=True) (making a lsit of all the elements in each cell) and then using i = ref['href'] to grab the hyperlink. If the hyperlinke is there we append it to the row we created in the preceeding line. If not we move on to extract just the text with i = tr.text and append the text to the row. We append each row to the list l we created above.\nMaking a dataframe out of the list of lists # Now as we append each row to the l list, we create a massive list of lists, where each element is the row of a table/dataframe. This is something we can feed to Pandas to create a dataframe, which is exactly what df = pd.DataFrame(l).\nAnd there you have your website table in pandas dataframe.\nSave the dataframe to file # You can save your dataframe to file with df.to_csv('Onekp_data.tsv',sep='\\t').\n"}]